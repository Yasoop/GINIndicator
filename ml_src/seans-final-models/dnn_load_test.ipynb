{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Unemployment Rate: 6.61%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def load_preprocessor(params_file):\n",
    "    \"\"\"Load preprocessing parameters from saved file.\"\"\"\n",
    "    with open(params_file, 'r') as f:\n",
    "        params = json.load(f)\n",
    "    return params\n",
    "\n",
    "def validate_input_data(data, params):\n",
    "    \"\"\"Validate that input data has all required columns.\"\"\"\n",
    "    required_columns = params['numerical_features'] + params['categorical_features']\n",
    "    missing_columns = [col for col in required_columns if col not in data.columns]\n",
    "    \n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Check for any NaN values\n",
    "    if data.isnull().any().any():\n",
    "        raise ValueError(\"Input data contains NaN values. Please clean your data first.\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "def preprocess_data(data, params):\n",
    "    \"\"\"Manually apply preprocessing transformations.\"\"\"\n",
    "    # Validate input first\n",
    "    validate_input_data(data, params)\n",
    "    \n",
    "    # Separate numerical and categorical data\n",
    "    numerical_data = data[params['numerical_features']]\n",
    "    categorical_data = data[params['categorical_features']]\n",
    "    \n",
    "    # Apply StandardScaler transformation manually\n",
    "    scaler_mean = np.array(params['scaler_mean'])\n",
    "    scaler_scale = np.array(params['scaler_scale'])\n",
    "    numerical_scaled = (numerical_data.values - scaler_mean) / scaler_scale\n",
    "    \n",
    "    # Apply OneHotEncoder transformation manually\n",
    "    categorical_encoded = []\n",
    "    for i, feature in enumerate(params['categorical_features']):\n",
    "        feature_categories = params['encoder_categories'][i]\n",
    "        feature_values = categorical_data[feature].values\n",
    "        \n",
    "        # Create one-hot encoding for this feature\n",
    "        encoded_feature = np.zeros((len(feature_values), len(feature_categories)))\n",
    "        for j, value in enumerate(feature_values):\n",
    "            if value in feature_categories:\n",
    "                category_idx = feature_categories.index(value)\n",
    "                encoded_feature[j, category_idx] = 1\n",
    "            # If unknown category, all zeros (handle_unknown='ignore' behavior)\n",
    "        \n",
    "        categorical_encoded.append(encoded_feature)\n",
    "    \n",
    "    # Combine numerical and categorical features\n",
    "    if categorical_encoded:\n",
    "        categorical_combined = np.hstack(categorical_encoded)\n",
    "        processed_data = np.hstack([numerical_scaled, categorical_combined])\n",
    "    else:\n",
    "        processed_data = numerical_scaled\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def predict_unemployment(new_data, model_path='Unemployment_AI_Optimized.keras', \n",
    "                        params_path='preprocessing_params_optimized.json'):\n",
    "    \"\"\"\n",
    "    Make unemployment prediction on new data.\n",
    "    \n",
    "    Args:\n",
    "        new_data: DataFrame with same columns as training data (except target)\n",
    "        model_path: Path to saved Keras model\n",
    "        params_path: Path to preprocessing parameters JSON\n",
    "    \n",
    "    Returns:\n",
    "        Predicted unemployment rate\n",
    "    \"\"\"\n",
    "    # Load model and preprocessing parameters\n",
    "    model = load_model(model_path)\n",
    "    params = load_preprocessor(params_path)\n",
    "    \n",
    "    # Preprocess the new data\n",
    "    new_data_processed = preprocess_data(new_data, params)\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(new_data_processed, verbose=0)\n",
    "    \n",
    "    return prediction.flatten()[0]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example new data point\n",
    "    new_data = pd.DataFrame({\n",
    "        'Region': ['Europe and Central Asia'],  \n",
    "        'Trade union density': [78.699997],\n",
    "        'Combined corporate income tax rate': [28.0],\n",
    "        'Education spending': [0.0734319847255705],\n",
    "        'Health spending': [0.0631525528524754],\n",
    "        'Housing spending': [0.0057497428086187],\n",
    "        'Community development spending': [0.0025634702523358],\n",
    "        'IRLT': [5.1075],\n",
    "        'Population, total': [8895960.0],\n",
    "        'GDP per capita (current US$)': [27259.4806735435],\n",
    "        'Inflation, consumer prices (annual %)': [2.40595834145438],\n",
    "        'Gini index': [26.5]\n",
    "    })\n",
    "    \n",
    "    # Make prediction\n",
    "    try:\n",
    "        predicted_unemployment = predict_unemployment(new_data)\n",
    "        print(f\"Predicted Unemployment Rate: {predicted_unemployment:.2f}%\")\n",
    "        \n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Could not find required file - {e}\")\n",
    "        print(\"Make sure both 'Unemployment_AI_Optimized.keras' and 'preprocessing_params_optimized.json' are in the same directory\")\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Data validation error: {e}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error making prediction: {e}\")\n",
    "        print(\"Please check your input data format and file paths\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tempdoctftwo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
