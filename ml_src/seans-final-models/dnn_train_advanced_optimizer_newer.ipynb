{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *NOTE: this is the updated optimizer for training our DNN*\n",
    "# Training of Final DNN\n",
    "This jupyter notebook file is where the final version of the DNN is trained and saved. It is written to be fully reproducible.\n",
    "\n",
    "I created a new env to run this file and its sister file (dnn_load_test.ipynb). It's probably easiest to create a new conda env using this command:\n",
    "\n",
    "and this YAML file content:\n",
    "\n",
    "``` yaml\n",
    "name: consensus-tf\n",
    "channels:\n",
    "  - defaults\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.11.5\n",
    "  - matplotlib=3.10\n",
    "  - scikit-learn=1.6.1\n",
    "  - tensorflow=2.12.0\n",
    "  - notebook=7.3.2\n",
    "  - pandas=2.2.3\n",
    "```\n",
    "\n",
    "### Extra note: If you do edit this file and yield a new model, you'll have to copy/paste the json and keras files that save the preprocessor and weights over to the api/backend/ml_models/deep_neural_network folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SIMPLIFIED HYPERPARAMETER OPTIMIZATION\n",
      "============================================================\n",
      "Training samples: 320\n",
      "Validation samples: 80\n",
      "\n",
      "Testing 8 configurations including your ORIGINAL model\n",
      "================================================================================\n",
      "üî• Testing YOUR ORIGINAL MODEL: [128, 64, 32, 16] with very_light regularization, batch_size=8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-13 02:43:37.498839: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSE: 3.955 | MAE: 1.31 | R¬≤: 0.784 | Overfitting: 2.30\n",
      "Testing: [256, 128, 64] with light regularization, batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSE: 4.151 | MAE: 1.27 | R¬≤: 0.773 | Overfitting: 3.46\n",
      "Testing: [256, 128, 64] with moderate regularization, batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSE: 3.500 | MAE: 1.22 | R¬≤: 0.809 | Overfitting: 2.90\n",
      "Testing: [224, 112, 56] with light regularization, batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSE: 3.507 | MAE: 1.16 | R¬≤: 0.808 | Overfitting: 2.84\n",
      "Testing: [224, 112, 56] with moderate regularization, batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSE: 3.793 | MAE: 1.20 | R¬≤: 0.793 | Overfitting: 3.23\n",
      "Testing: [256, 128, 64, 32] with light regularization, batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSE: 3.367 | MAE: 1.03 | R¬≤: 0.816 | Overfitting: 2.93\n",
      "Testing: [256, 128, 64, 32] with moderate regularization, batch_size=32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  MSE: 4.312 | MAE: 1.24 | R¬≤: 0.764 | Overfitting: 3.60\n",
      "Testing: [128, 64, 32, 16] with moderate regularization, batch_size=32\n",
      "  MSE: 3.754 | MAE: 1.24 | R¬≤: 0.795 | Overfitting: 3.22\n",
      "\n",
      "==========================================================================================\n",
      "RESULTS SUMMARY (Best to Worst) - ALL EVALUATED ON UNSEEN VALIDATION DATA\n",
      "==========================================================================================\n",
      "Rank Architecture         Reg        Batch MSE    MAE    R¬≤     Overfit\n",
      "------------------------------------------------------------------------------------------\n",
      "1    [256,128,64,32]      light      32    3.367  1.03   0.816  2.93   \n",
      "2    [256,128,64]         moderate   32    3.500  1.22   0.809  2.90   \n",
      "3    [224,112,56]         light      32    3.507  1.16   0.808  2.84   \n",
      "4    [128,64,32,16]       moderate   32    3.754  1.24   0.795  3.22   \n",
      "5    [224,112,56]         moderate   32    3.793  1.20   0.793  3.23   \n",
      "üî•6   [128,64,32,16]       very_light 8     3.955  1.31   0.784  2.30   \n",
      "7    [256,128,64]         light      32    4.151  1.27   0.773  3.46   \n",
      "8    [256,128,64,32]      moderate   32    4.312  1.24   0.764  3.60   \n",
      "\n",
      "üèÜ OVERALL WINNER:\n",
      "   Architecture: [256, 128, 64, 32]\n",
      "   Regularization: light (dropout=0.05, L2=0.0001)\n",
      "   Batch Size: 32\n",
      "   Performance: MSE=3.367, MAE=1.03, R¬≤=0.816\n",
      "   Overfitting Gap: 2.93\n",
      "\n",
      "üìä YOUR ORIGINAL MODEL ANALYSIS:\n",
      "   Ranked: #6 out of 8\n",
      "   FAIR R¬≤ Score: 0.784 (vs your reported 0.83)\n",
      "   MSE: 3.955\n",
      "   MAE: 1.31\n",
      "   üìä Your original model is solid but others perform better\n",
      "   üìà Winner is 0.587 MSE better (14.9% improvement)\n",
      "\n",
      "üí° KEY INSIGHT:\n",
      "   Original reported R¬≤ = 0.83 (evaluated on training data - inflated)\n",
      "   True R¬≤ on unseen data = 0.784 (fair evaluation)\n",
      "   Difference = 0.046 (due to data leakage)\n",
      "\n",
      "   ‚úÖ Good generalization\n",
      "\n",
      "============================================================\n",
      "RECOMMENDATION\n",
      "============================================================\n",
      "üéØ EXCELLENT MODEL - Ready for deployment!\n",
      "\n",
      "MAE of 1.0 means predictions are typically off by ¬±1.0 percentage points\n",
      "R¬≤ of 0.816 means the model explains 81.6% of unemployment variance\n",
      "\n",
      "============================================================\n",
      "TRAINING FINAL MODEL\n",
      "============================================================\n",
      "Training winning configuration on full dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "13/13 [==============================] - 1s 2ms/step - loss: 75.3458 - mae: 7.8830\n",
      "Epoch 2/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 69.4797 - mae: 7.7981\n",
      "Epoch 3/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 65.8547 - mae: 7.6831\n",
      "Epoch 4/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 63.3904 - mae: 7.5902\n",
      "Epoch 5/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 60.3596 - mae: 7.4377\n",
      "Epoch 6/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 59.2168 - mae: 7.3385\n",
      "Epoch 7/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 56.4215 - mae: 7.2033\n",
      "Epoch 8/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 53.8053 - mae: 7.0531\n",
      "Epoch 9/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 51.8909 - mae: 6.8996\n",
      "Epoch 10/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 48.8812 - mae: 6.7252\n",
      "Epoch 11/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 47.3963 - mae: 6.6107\n",
      "Epoch 12/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 43.1502 - mae: 6.3251\n",
      "Epoch 13/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 39.7773 - mae: 6.0646\n",
      "Epoch 14/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 37.4863 - mae: 5.8698\n",
      "Epoch 15/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 34.9522 - mae: 5.6345\n",
      "Epoch 16/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 32.2844 - mae: 5.4141\n",
      "Epoch 17/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 28.6639 - mae: 5.1159\n",
      "Epoch 18/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 26.5016 - mae: 4.8160\n",
      "Epoch 19/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 23.4851 - mae: 4.5375\n",
      "Epoch 20/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 21.4370 - mae: 4.2677\n",
      "Epoch 21/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 19.0171 - mae: 3.9837\n",
      "Epoch 22/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 17.1101 - mae: 3.7185\n",
      "Epoch 23/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 14.1281 - mae: 3.3989\n",
      "Epoch 24/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 12.2230 - mae: 3.1260\n",
      "Epoch 25/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 10.5874 - mae: 2.8586\n",
      "Epoch 26/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 9.4613 - mae: 2.6422\n",
      "Epoch 27/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 7.9694 - mae: 2.3917\n",
      "Epoch 28/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.8041 - mae: 2.1546\n",
      "Epoch 29/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 6.1097 - mae: 2.0632\n",
      "Epoch 30/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.5175 - mae: 1.8932\n",
      "Epoch 31/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 5.1926 - mae: 1.8608\n",
      "Epoch 32/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 4.3011 - mae: 1.6014\n",
      "Epoch 33/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 4.4540 - mae: 1.6182\n",
      "Epoch 34/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.9937 - mae: 1.5612\n",
      "Epoch 35/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.4583 - mae: 1.4433\n",
      "Epoch 36/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.3639 - mae: 1.3829\n",
      "Epoch 37/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 3.1444 - mae: 1.3843\n",
      "Epoch 38/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6930 - mae: 1.2495\n",
      "Epoch 39/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.7019 - mae: 1.3003\n",
      "Epoch 40/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.9800 - mae: 1.2866\n",
      "Epoch 41/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.9326 - mae: 1.2876\n",
      "Epoch 42/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6472 - mae: 1.2014\n",
      "Epoch 43/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.6910 - mae: 1.2301\n",
      "Epoch 44/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.7992 - mae: 1.2611\n",
      "Epoch 45/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.8119 - mae: 1.2618\n",
      "Epoch 46/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.5176 - mae: 1.2023\n",
      "Epoch 47/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.4299 - mae: 1.2096\n",
      "Epoch 48/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 3.0505 - mae: 1.2966\n",
      "Epoch 49/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3180 - mae: 1.1429\n",
      "Epoch 50/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.7453 - mae: 1.2813\n",
      "Epoch 51/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.4223 - mae: 1.2149\n",
      "Epoch 52/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2814 - mae: 1.1546\n",
      "Epoch 53/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.5131 - mae: 1.1720\n",
      "Epoch 54/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 3.0020 - mae: 1.2814\n",
      "Epoch 55/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.2847 - mae: 1.1437\n",
      "Epoch 56/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.4006 - mae: 1.1865\n",
      "Epoch 57/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9924 - mae: 1.0629\n",
      "Epoch 58/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.3682 - mae: 1.1698\n",
      "Epoch 59/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.3164 - mae: 1.1537\n",
      "Epoch 60/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.2718 - mae: 1.1439\n",
      "Epoch 61/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.5942 - mae: 0.9704\n",
      "Epoch 62/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.9142 - mae: 1.0721\n",
      "Epoch 63/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.1814 - mae: 1.1221\n",
      "Epoch 64/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.4466 - mae: 1.2015\n",
      "Epoch 65/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.5255 - mae: 1.1825\n",
      "Epoch 66/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.0199 - mae: 1.1041\n",
      "Epoch 67/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.2057 - mae: 1.1377\n",
      "Epoch 68/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.8977 - mae: 0.9775\n",
      "Epoch 69/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.8167 - mae: 1.0244\n",
      "Epoch 70/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.1705 - mae: 1.1183\n",
      "Epoch 71/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.2697 - mae: 1.1575\n",
      "Epoch 72/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.9221 - mae: 1.0714\n",
      "Epoch 73/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.7256 - mae: 1.0018\n",
      "Epoch 74/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.2540 - mae: 1.1254\n",
      "Epoch 75/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.6263 - mae: 1.2262\n",
      "Epoch 76/300\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 1.9854 - mae: 1.0360\n",
      "Epoch 77/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.6478 - mae: 0.9978\n",
      "Epoch 78/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 2.0731 - mae: 1.0671\n",
      "Epoch 79/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.9594 - mae: 1.0784\n",
      "Epoch 80/300\n",
      "13/13 [==============================] - 0s 1ms/step - loss: 1.6589 - mae: 0.9618\n",
      "Epoch 81/300\n",
      " 1/13 [=>............................] - ETA: 0s - loss: 2.4551 - mae: 1.2256Restoring model weights from the end of the best epoch: 61.\n",
      "13/13 [==============================] - 0s 2ms/step - loss: 2.3079 - mae: 1.1453\n",
      "Epoch 81: early stopping\n",
      "\n",
      "‚úÖ COMPLETE!\n",
      "   Optimized model saved: 'Unemployment_AI_Optimized.keras'\n",
      "   Preprocessing saved: 'preprocessing_params_optimized.json'\n",
      "   Winning config: [256, 128, 64, 32] with light reg, batch_size=32\n",
      "   Expected performance: MAE ‚âà 1.0, R¬≤ ‚âà 0.816\n",
      "\n",
      "üîç ORIGINAL MODEL VERDICT:\n",
      "   Your original model was good, but optimization found better configs\n",
      "   The 0.83 R¬≤ you saw was inflated due to evaluating on training data\n",
      "   True performance: R¬≤ = 0.784 on unseen data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "RANDOM_SEED = 11\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SIMPLIFIED HYPERPARAMETER OPTIMIZATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load and prepare data\n",
    "df = pd.read_csv('/Users/seanblundin/Documents/courses/cs3200/GINIndicator/datasets/MEGAFRAME_CLEANEDV2.csv')\n",
    "X = df.drop(columns=['UNEMP', 'Reference area', 'REF_AREA', 'TIME_PERIOD'])\n",
    "y = df['UNEMP']\n",
    "\n",
    "categorical_features = ['Region']\n",
    "numerical_features = X.columns.difference(categorical_features)\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numerical_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
    "])\n",
    "\n",
    "X_processed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Train/validation split (keeping test simple)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y, test_size=0.2, random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "print(f\"Validation samples: {len(X_val)}\")\n",
    "\n",
    "def create_and_evaluate_model(architecture, regularization_level, batch_size, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"Create, train, and evaluate a model configuration\"\"\"\n",
    "    \n",
    "    # Set regularization based on level\n",
    "    if regularization_level == 'very_light':\n",
    "        dropout_rate = 0.02\n",
    "        l2_reg = 0.0001\n",
    "    elif regularization_level == 'light':\n",
    "        dropout_rate = 0.05\n",
    "        l2_reg = 0.0001\n",
    "    else:  # moderate\n",
    "        dropout_rate = 0.1\n",
    "        l2_reg = 0.001\n",
    "    \n",
    "    # Create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(architecture[0], activation='relu', kernel_regularizer=l2(l2_reg), \n",
    "                   input_shape=(X_train.shape[1],)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    for units in architecture[1:]:\n",
    "        model.add(Dense(units, activation='relu', kernel_regularizer=l2(l2_reg)))\n",
    "        model.add(BatchNormalization())\n",
    "        if units > 16:  # Only dropout on larger layers (matching your original logic)\n",
    "            model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(1))  # Output layer\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    # Train with callbacks\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=0)\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=300,  # Match your original epochs\n",
    "        batch_size=batch_size,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Evaluate on validation set only (fair comparison)\n",
    "    val_pred = model.predict(X_val, verbose=0)\n",
    "    train_pred = model.predict(X_train, verbose=0)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    val_mse = mean_squared_error(y_val, val_pred)\n",
    "    val_mae = mean_absolute_error(y_val, val_pred)\n",
    "    val_r2 = r2_score(y_val, val_pred)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    overfitting_gap = val_mse - train_mse\n",
    "    \n",
    "    return {\n",
    "        'architecture': architecture,\n",
    "        'regularization': regularization_level,\n",
    "        'batch_size': batch_size,\n",
    "        'val_mse': val_mse,\n",
    "        'val_mae': val_mae,\n",
    "        'val_r2': val_r2,\n",
    "        'overfitting_gap': overfitting_gap,\n",
    "        'dropout': dropout_rate,\n",
    "        'l2_reg': l2_reg\n",
    "    }\n",
    "\n",
    "# Define architectures to test (including your original)\n",
    "architectures = [\n",
    "    [128, 64, 32, 16],   # YOUR ORIGINAL MODEL\n",
    "    [256, 128, 64],      # Previous optimization winner\n",
    "    [224, 112, 56],      # Slightly smaller\n",
    "    [256, 128, 64, 32],  # Best from recent results\n",
    "]\n",
    "\n",
    "regularization_levels = ['very_light', 'light', 'moderate']\n",
    "\n",
    "# Test configurations\n",
    "test_configs = []\n",
    "\n",
    "# Test your original model exactly as it was\n",
    "test_configs.append({\n",
    "    'architecture': [128, 64, 32, 16],\n",
    "    'regularization': 'very_light',\n",
    "    'batch_size': 8  # Your original batch size\n",
    "})\n",
    "\n",
    "# Test other architectures with different settings\n",
    "for arch in architectures:\n",
    "    if arch != [128, 64, 32, 16]:  # Skip original since we already added it\n",
    "        for reg_level in ['light', 'moderate']:\n",
    "            test_configs.append({\n",
    "                'architecture': arch,\n",
    "                'regularization': reg_level,\n",
    "                'batch_size': 32\n",
    "            })\n",
    "\n",
    "# Also test your original architecture with modern settings\n",
    "test_configs.append({\n",
    "    'architecture': [128, 64, 32, 16],\n",
    "    'regularization': 'moderate',\n",
    "    'batch_size': 32\n",
    "})\n",
    "\n",
    "print(f\"\\nTesting {len(test_configs)} configurations including your ORIGINAL model\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Run experiments\n",
    "results = []\n",
    "for i, config in enumerate(test_configs):\n",
    "    arch = config['architecture']\n",
    "    reg_level = config['regularization']\n",
    "    batch_size = config['batch_size']\n",
    "    \n",
    "    # Highlight original model\n",
    "    if arch == [128, 64, 32, 16] and reg_level == 'very_light' and batch_size == 8:\n",
    "        print(f\"üî• Testing YOUR ORIGINAL MODEL: {arch} with {reg_level} regularization, batch_size={batch_size}\")\n",
    "    else:\n",
    "        print(f\"Testing: {arch} with {reg_level} regularization, batch_size={batch_size}\")\n",
    "        \n",
    "    result = create_and_evaluate_model(arch, reg_level, batch_size, X_train, y_train, X_val, y_val)\n",
    "    results.append(result)\n",
    "    print(f\"  MSE: {result['val_mse']:.3f} | MAE: {result['val_mae']:.2f} | R¬≤: {result['val_r2']:.3f} | Overfitting: {result['overfitting_gap']:.2f}\")\n",
    "\n",
    "# Sort results by validation MSE\n",
    "results.sort(key=lambda x: x['val_mse'])\n",
    "\n",
    "print(f\"\\n{'='*90}\")\n",
    "print(\"RESULTS SUMMARY (Best to Worst) - ALL EVALUATED ON UNSEEN VALIDATION DATA\")\n",
    "print(f\"{'='*90}\")\n",
    "print(f\"{'Rank':<4} {'Architecture':<20} {'Reg':<10} {'Batch':<5} {'MSE':<6} {'MAE':<6} {'R¬≤':<6} {'Overfit':<7}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    arch_str = str(result['architecture']).replace(' ', '')\n",
    "    \n",
    "    # Highlight original model in results\n",
    "    if (result['architecture'] == [128, 64, 32, 16] and \n",
    "        result['regularization'] == 'very_light' and \n",
    "        result['batch_size'] == 8):\n",
    "        rank_str = f\"üî•{i+1}\"\n",
    "    else:\n",
    "        rank_str = str(i+1)\n",
    "    \n",
    "    print(f\"{rank_str:<4} {arch_str:<20} {result['regularization']:<10} {result['batch_size']:<5} \"\n",
    "          f\"{result['val_mse']:<6.3f} {result['val_mae']:<6.2f} {result['val_r2']:<6.3f} {result['overfitting_gap']:<7.2f}\")\n",
    "\n",
    "# Find your original model in results\n",
    "original_result = None\n",
    "for result in results:\n",
    "    if (result['architecture'] == [128, 64, 32, 16] and \n",
    "        result['regularization'] == 'very_light' and \n",
    "        result['batch_size'] == 8):\n",
    "        original_result = result\n",
    "        break\n",
    "\n",
    "original_rank = results.index(original_result) + 1 if original_result else None\n",
    "\n",
    "# Identify winner\n",
    "winner = results[0]\n",
    "print(f\"\\nüèÜ OVERALL WINNER:\")\n",
    "print(f\"   Architecture: {winner['architecture']}\")\n",
    "print(f\"   Regularization: {winner['regularization']} (dropout={winner['dropout']}, L2={winner['l2_reg']})\")\n",
    "print(f\"   Batch Size: {winner['batch_size']}\")\n",
    "print(f\"   Performance: MSE={winner['val_mse']:.3f}, MAE={winner['val_mae']:.2f}, R¬≤={winner['val_r2']:.3f}\")\n",
    "print(f\"   Overfitting Gap: {winner['overfitting_gap']:.2f}\")\n",
    "\n",
    "# Special analysis of your original model\n",
    "if original_result:\n",
    "    print(f\"\\nüìä YOUR ORIGINAL MODEL ANALYSIS:\")\n",
    "    print(f\"   Ranked: #{original_rank} out of {len(results)}\")\n",
    "    print(f\"   FAIR R¬≤ Score: {original_result['val_r2']:.3f} (vs your reported 0.83)\")\n",
    "    print(f\"   MSE: {original_result['val_mse']:.3f}\")\n",
    "    print(f\"   MAE: {original_result['val_mae']:.2f}\")\n",
    "    \n",
    "    if original_rank == 1:\n",
    "        print(f\"   üéâ Your original model IS the winner!\")\n",
    "    elif original_rank <= 3:\n",
    "        print(f\"   üëç Your original model performs very well (top 3)\")\n",
    "        winner_gap = winner['val_mse'] - original_result['val_mse']\n",
    "        print(f\"   üìà Winner is only {winner_gap:.3f} MSE better\")\n",
    "    else:\n",
    "        print(f\"   üìä Your original model is solid but others perform better\")\n",
    "        winner_gap = original_result['val_mse'] - winner['val_mse']\n",
    "        print(f\"   üìà Winner is {winner_gap:.3f} MSE better ({winner_gap/original_result['val_mse']*100:.1f}% improvement)\")\n",
    "    \n",
    "    print(f\"\\nüí° KEY INSIGHT:\")\n",
    "    print(f\"   Original reported R¬≤ = 0.83 (evaluated on training data - inflated)\")\n",
    "    print(f\"   True R¬≤ on unseen data = {original_result['val_r2']:.3f} (fair evaluation)\")\n",
    "    print(f\"   Difference = {0.83 - original_result['val_r2']:.3f} (due to data leakage)\")\n",
    "\n",
    "# Quick overfitting assessment\n",
    "if winner['overfitting_gap'] < 1.0:\n",
    "    print(f\"\\n   ‚úÖ Excellent generalization\")\n",
    "elif winner['overfitting_gap'] < 3.0:\n",
    "    print(f\"\\n   ‚úÖ Good generalization\") \n",
    "elif winner['overfitting_gap'] < 5.0:\n",
    "    print(f\"\\n   ‚ö†Ô∏è  Moderate overfitting\")\n",
    "else:\n",
    "    print(f\"\\n   ‚ùå High overfitting - consider more regularization\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"RECOMMENDATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "if winner['val_r2'] > 0.8:\n",
    "    print(\"üéØ EXCELLENT MODEL - Ready for deployment!\")\n",
    "elif winner['val_r2'] > 0.7:\n",
    "    print(\"üëç GOOD MODEL - Should work well for predictions\")\n",
    "elif winner['val_r2'] > 0.6:\n",
    "    print(\"ü§î DECENT MODEL - Consider more data or features\")\n",
    "else:\n",
    "    print(\"üòü WEAK MODEL - May need fundamental changes\")\n",
    "\n",
    "print(f\"\\nMAE of {winner['val_mae']:.1f} means predictions are typically off by ¬±{winner['val_mae']:.1f} percentage points\")\n",
    "print(f\"R¬≤ of {winner['val_r2']:.3f} means the model explains {winner['val_r2']*100:.1f}% of unemployment variance\")\n",
    "\n",
    "# Train final model on full dataset\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"TRAINING FINAL MODEL\")  \n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"Training winning configuration on full dataset...\")\n",
    "\n",
    "# Use winning parameters for final model\n",
    "final_model = Sequential()\n",
    "final_model.add(Dense(winner['architecture'][0], activation='relu', \n",
    "                     kernel_regularizer=l2(winner['l2_reg']), input_shape=(X_processed.shape[1],)))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(Dropout(winner['dropout']))\n",
    "\n",
    "for units in winner['architecture'][1:]:\n",
    "    final_model.add(Dense(units, activation='relu', kernel_regularizer=l2(winner['l2_reg'])))\n",
    "    final_model.add(BatchNormalization())\n",
    "    if units > 16:  # Match the logic from evaluation\n",
    "        final_model.add(Dropout(winner['dropout']))\n",
    "\n",
    "final_model.add(Dense(1))\n",
    "final_model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train on full dataset\n",
    "early_stop = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True, verbose=1)\n",
    "final_model.fit(X_processed, y, epochs=300, batch_size=winner['batch_size'], callbacks=[early_stop], verbose=1)\n",
    "\n",
    "# Save model and preprocessing parameters\n",
    "final_model.save('Unemployment_AI_Optimized.keras')\n",
    "\n",
    "# Save preprocessing parameters  \n",
    "import json\n",
    "preprocessing_params = {\n",
    "    'numerical_features': list(numerical_features),\n",
    "    'categorical_features': categorical_features,\n",
    "    'scaler_mean': preprocessor.named_transformers_['num'].mean_.tolist(),\n",
    "    'scaler_scale': preprocessor.named_transformers_['num'].scale_.tolist(),\n",
    "    'encoder_categories': [cat.tolist() for cat in preprocessor.named_transformers_['cat'].categories_]\n",
    "}\n",
    "\n",
    "with open('preprocessing_params_optimized.json', 'w') as f:\n",
    "    json.dump(preprocessing_params, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ COMPLETE!\")\n",
    "print(f\"   Optimized model saved: 'Unemployment_AI_Optimized.keras'\")\n",
    "print(f\"   Preprocessing saved: 'preprocessing_params_optimized.json'\")\n",
    "print(f\"   Winning config: {winner['architecture']} with {winner['regularization']} reg, batch_size={winner['batch_size']}\")\n",
    "print(f\"   Expected performance: MAE ‚âà {winner['val_mae']:.1f}, R¬≤ ‚âà {winner['val_r2']:.3f}\")\n",
    "\n",
    "if original_result and original_rank:\n",
    "    print(f\"\\nüîç ORIGINAL MODEL VERDICT:\")\n",
    "    if original_rank <= 2:\n",
    "        print(f\"   Your original model was excellent! (Ranked #{original_rank})\")\n",
    "    else:\n",
    "        print(f\"   Your original model was good, but optimization found better configs\")\n",
    "    print(f\"   The 0.83 R¬≤ you saw was inflated due to evaluating on training data\")\n",
    "    print(f\"   True performance: R¬≤ = {original_result['val_r2']:.3f} on unseen data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp-finaltest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
